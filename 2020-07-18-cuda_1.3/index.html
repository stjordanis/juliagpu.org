<!doctype html>

<html lang="en" class="h-100">
  <head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta content="index, follow" name="robots">
  <meta name="generator" content="Hugo 0.77.0" />
  <link rel="stylesheet" href="https://juliagpu.org/css/bootstrap.min.css">

  
  
  
  <title>CUDA.jl 1.3 - Multi-device programming Â· JuliaGPU</title>
  <style>
.container {
  max-width: 700px;
}
#nav-border {
  border-bottom: 1px solid #212529;
}
#main {
  margin-top: 1em;
  margin-bottom: 4em;
}
#home-jumbotron {
  background-color: inherit;
}
#footer .container {
  padding: 1em 0;
}
#footer a {
  color: inherit;
  text-decoration: underline;
}
.font-125 {
  font-size: 125%;
}
.tag-btn {
  margin-bottom: 0.3em;
}
pre {
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
  padding: 16px;
}
pre code {
  padding: 0;
  font-size: inherit;
  color: inherit;
  background-color: transparent;
  border-radius: 0;
}
code {
  padding: 2px 4px;
  font-size: 90%;
  color: #c7254e;
  background-color: #f9f2f4;
  border-radius: 4px;
}
img,
iframe,
embed,
video,
audio {
  max-width: 100%;
}
.card-img,
.card-img-top,
.card-img-bottom {
  width: initial;
}
</style>

</head>

  <body class="d-flex flex-column h-100">
    <div id="nav-border" class="container">
  <nav class="navbar navbar-expand-lg navbar-light justify-content-center">
    
    
    <ul class="navbar-nav">
      
        
        
        
        
        
        

        
          <li class="nav-item ">
            <a class="nav-link" href="/">
              
                
                <i data-feather="home"></i> 
              
              Home
            </a>
          </li>
        
      
        
        
        
        
        
        

        
          <li class="nav-item active">
            <a class="nav-link" href="/post/">
              
                
                <i data-feather="file-text"></i> 
              
              Blog
            </a>
          </li>
        
      
        
        
        
        
        
        

        
          <li class="nav-item ">
            <a class="nav-link" href="/learn/">
              
                
                <i data-feather="book-open"></i> 
              
              Learn
            </a>
          </li>
        
      
        
        
        
        
        
        

        
          <li class="nav-item ">
            <a class="nav-link" href="/cuda/">
              
              NVIDIA CUDA
            </a>
          </li>
        
      
        
        
        
        
        
        

        
          <li class="nav-item ">
            <a class="nav-link" href="/rocm/">
              
              AMD ROCm
            </a>
          </li>
        
      
        
        
        
        
        
        

        
          <li class="nav-item ">
            <a class="nav-link" href="/other/">
              
              Other
            </a>
          </li>
        
      
    </ul>
  </nav>
</div>

    <div class="container">
      <main id="main">
        

<h1>CUDA.jl 1.3 - Multi-device programming</h1>



<i data-feather="calendar"></i> <time datetime="2020-07-18">Jul 18, 2020</time>




  <br>
  <i data-feather="edit-2"></i> Tim Besard


<br><br>

    <p>Today we&rsquo;re releasing CUDA.jl 1.3, with several new features. The most prominent
change is support for multiple GPUs within a single process.</p>
<h2 id="multi-gpu-programming">Multi-GPU programming</h2>
<p>With CUDA.jl 1.3, you can finally use multiple CUDA GPUs within a single process. To switch
devices you can call <code>device!</code>, query the current device with <code>device()</code>, or reset it using
<code>device_reset!()</code>:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-julia" data-lang="julia">julia<span style="color:#f92672">&gt;</span> collect(devices())
<span style="color:#ae81ff">9</span><span style="color:#f92672">-</span>element <span style="color:#66d9ef">Array</span>{CuDevice,<span style="color:#ae81ff">1</span>}<span style="color:#f92672">:</span>
 CuDevice(<span style="color:#ae81ff">0</span>)<span style="color:#f92672">:</span> Tesla V100<span style="color:#f92672">-</span>PCIE<span style="color:#f92672">-</span><span style="color:#ae81ff">32</span>GB
 CuDevice(<span style="color:#ae81ff">1</span>)<span style="color:#f92672">:</span> Tesla V100<span style="color:#f92672">-</span>PCIE<span style="color:#f92672">-</span><span style="color:#ae81ff">32</span>GB
 CuDevice(<span style="color:#ae81ff">2</span>)<span style="color:#f92672">:</span> Tesla V100<span style="color:#f92672">-</span>PCIE<span style="color:#f92672">-</span><span style="color:#ae81ff">32</span>GB
 CuDevice(<span style="color:#ae81ff">3</span>)<span style="color:#f92672">:</span> Tesla V100<span style="color:#f92672">-</span>PCIE<span style="color:#f92672">-</span><span style="color:#ae81ff">32</span>GB
 CuDevice(<span style="color:#ae81ff">4</span>)<span style="color:#f92672">:</span> Tesla V100<span style="color:#f92672">-</span>PCIE<span style="color:#f92672">-</span><span style="color:#ae81ff">16</span>GB
 CuDevice(<span style="color:#ae81ff">5</span>)<span style="color:#f92672">:</span> Tesla P100<span style="color:#f92672">-</span>PCIE<span style="color:#f92672">-</span><span style="color:#ae81ff">16</span>GB
 CuDevice(<span style="color:#ae81ff">6</span>)<span style="color:#f92672">:</span> Tesla P100<span style="color:#f92672">-</span>PCIE<span style="color:#f92672">-</span><span style="color:#ae81ff">16</span>GB
 CuDevice(<span style="color:#ae81ff">7</span>)<span style="color:#f92672">:</span> GeForce GTX <span style="color:#ae81ff">1080</span> Ti
 CuDevice(<span style="color:#ae81ff">8</span>)<span style="color:#f92672">:</span> GeForce GTX <span style="color:#ae81ff">1080</span> Ti

julia<span style="color:#f92672">&gt;</span> device!(<span style="color:#ae81ff">5</span>)

julia<span style="color:#f92672">&gt;</span> device()
CuDevice(<span style="color:#ae81ff">5</span>)<span style="color:#f92672">:</span> Tesla P100<span style="color:#f92672">-</span>PCIE<span style="color:#f92672">-</span><span style="color:#ae81ff">16</span>GB
</code></pre></div><p>Let&rsquo;s define a kernel to show this really works:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-julia" data-lang="julia">julia<span style="color:#f92672">&gt;</span> <span style="color:#66d9ef">function</span> kernel()
           dev <span style="color:#f92672">=</span> <span style="color:#66d9ef">Ref</span>{<span style="color:#66d9ef">Cint</span>}()
           CUDA<span style="color:#f92672">.</span>cudaGetDevice(dev)
           <span style="color:#a6e22e">@cuprintln</span>(<span style="color:#e6db74">&#34;Running on device </span><span style="color:#e6db74">$</span>(dev[])<span style="color:#e6db74">&#34;</span>)
           <span style="color:#66d9ef">return</span>
       <span style="color:#66d9ef">end</span>

julia<span style="color:#f92672">&gt;</span> <span style="color:#a6e22e">@cuda</span> kernel()
Running on device <span style="color:#ae81ff">5</span>

julia<span style="color:#f92672">&gt;</span> device!(<span style="color:#ae81ff">0</span>)

julia<span style="color:#f92672">&gt;</span> device()
CuDevice(<span style="color:#ae81ff">0</span>)<span style="color:#f92672">:</span> Tesla V100<span style="color:#f92672">-</span>PCIE<span style="color:#f92672">-</span><span style="color:#ae81ff">32</span>GB

julia<span style="color:#f92672">&gt;</span> <span style="color:#a6e22e">@cuda</span> kernel()
Running on device <span style="color:#ae81ff">0</span>
</code></pre></div><p>Memory allocations, like <code>CuArray</code>s, are implicitly bound to the device they
were allocated on. That means you should take care to only use an array when the
owning device is active, or you will run into errors:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-julia" data-lang="julia">julia<span style="color:#f92672">&gt;</span> device()
CuDevice(<span style="color:#ae81ff">0</span>)<span style="color:#f92672">:</span> Tesla V100<span style="color:#f92672">-</span>PCIE<span style="color:#f92672">-</span><span style="color:#ae81ff">32</span>GB

julia<span style="color:#f92672">&gt;</span> a <span style="color:#f92672">=</span> CUDA<span style="color:#f92672">.</span>rand(<span style="color:#ae81ff">1</span>)
<span style="color:#ae81ff">1</span><span style="color:#f92672">-</span>element CuArray{<span style="color:#66d9ef">Float32</span>,<span style="color:#ae81ff">1</span>}<span style="color:#f92672">:</span>
 <span style="color:#ae81ff">0.6322775</span>

julia<span style="color:#f92672">&gt;</span> device!(<span style="color:#ae81ff">1</span>)

julia<span style="color:#f92672">&gt;</span> a
ERROR<span style="color:#f92672">:</span> CUDA error<span style="color:#f92672">:</span> an illegal memory access was encountered
</code></pre></div><p>Future improvements might make the array type device-aware.</p>
<h2 id="multitasking-and-multithreading">Multitasking and multithreading</h2>
<p>Dovetailing with the support for multiple GPUs, is the ability to use these GPUs
on separate Julia tasks and threads:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-julia" data-lang="julia">julia<span style="color:#f92672">&gt;</span> device!(<span style="color:#ae81ff">0</span>)

julia<span style="color:#f92672">&gt;</span> <span style="color:#a6e22e">@sync</span> <span style="color:#66d9ef">begin</span>
         <span style="color:#a6e22e">@async</span> <span style="color:#66d9ef">begin</span>
           device!(<span style="color:#ae81ff">1</span>)
           println(<span style="color:#e6db74">&#34;Working with </span><span style="color:#e6db74">$</span>(device())<span style="color:#e6db74"> on </span><span style="color:#e6db74">$</span>(current_task())<span style="color:#e6db74">&#34;</span>)
           yield()
           println(<span style="color:#e6db74">&#34;Back to device </span><span style="color:#e6db74">$</span>(device())<span style="color:#e6db74"> on </span><span style="color:#e6db74">$</span>(current_task())<span style="color:#e6db74">&#34;</span>)
         <span style="color:#66d9ef">end</span>
         <span style="color:#a6e22e">@async</span> <span style="color:#66d9ef">begin</span>
           device!(<span style="color:#ae81ff">2</span>)
           println(<span style="color:#e6db74">&#34;Working with </span><span style="color:#e6db74">$</span>(device())<span style="color:#e6db74"> on </span><span style="color:#e6db74">$</span>(current_task())<span style="color:#e6db74">&#34;</span>)
         <span style="color:#66d9ef">end</span>
       <span style="color:#66d9ef">end</span>
Working with CuDevice(<span style="color:#ae81ff">1</span>) on <span style="color:#66d9ef">Task</span> <span style="color:#960050;background-color:#1e0010">@</span><span style="color:#ae81ff">0x00007fc9e6a48010</span>
Working with CuDevice(<span style="color:#ae81ff">2</span>) on <span style="color:#66d9ef">Task</span> <span style="color:#960050;background-color:#1e0010">@</span><span style="color:#ae81ff">0x00007fc9e6a484f0</span>
Back to device CuDevice(<span style="color:#ae81ff">1</span>) on <span style="color:#66d9ef">Task</span> <span style="color:#960050;background-color:#1e0010">@</span><span style="color:#ae81ff">0x00007fc9e6a48010</span>

julia<span style="color:#f92672">&gt;</span> device()
CuDevice(<span style="color:#ae81ff">0</span>)<span style="color:#f92672">:</span> Tesla V100<span style="color:#f92672">-</span>PCIE<span style="color:#f92672">-</span><span style="color:#ae81ff">32</span>GB
</code></pre></div><p>Each task has its own local GPU state, such as the device it was bound to,
handles to libraries like CUBLAS or CUDNN (which means that each task can
configure libraries independently), etc.</p>
<h2 id="minor-features">Minor features</h2>
<p>CUDA.jl 1.3 also features some minor changes:</p>
<ul>
<li>Reinstated compatibility with Julia 1.3</li>
<li>Support for CUDA 11.0 Update 1</li>
<li>Support for CUDNN 8.0.2</li>
</ul>
<h2 id="known-issues">Known issues</h2>
<p>Several operations on sparse arrays have been broken since CUDA.jl 1.2, due to
the deprecations that were part of CUDA 11. The next version of CUDA.jl will
drop support for CUDA 10.0 or older, which will make it possible to use new
cuSPARSE APIs and add back missing functionality.</p>



      </main>
    </div>
    
<footer id="footer" class="mt-auto text-center text-muted">
  <div class="container">
    Made with <a href="https://gohugo.io/">Hugo</a> &amp; <a href="https://github.com/zwbetz-gh/vanilla-bootstrap-hugo-theme">Vanilla</a>
  </div>
</footer>

    <script src="https://juliagpu.org/js/feather.min.js"></script>
<script>
  feather.replace()
</script>


    
  
  <script>
  window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
  ga('create', 'UA-154489943-1', 'auto');
  ga('send', 'pageview');
  </script>
  <script async src='https://www.google-analytics.com/analytics.js'></script>
  

  </body>
</html>
