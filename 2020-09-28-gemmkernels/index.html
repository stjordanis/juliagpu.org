<!doctype html>

<html lang="en" class="h-100">
  <head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta content="index, follow" name="robots">
  <meta name="generator" content="Hugo 0.77.0" />
  <link rel="stylesheet" href="https://juliagpu.org/css/bootstrap.min.css">

  
  
  
  <title>Paper: Flexible Performant GEMM Kernels on GPUs Â· JuliaGPU</title>
  <style>
.container {
  max-width: 700px;
}
#nav-border {
  border-bottom: 1px solid #212529;
}
#main {
  margin-top: 1em;
  margin-bottom: 4em;
}
#home-jumbotron {
  background-color: inherit;
}
#footer .container {
  padding: 1em 0;
}
#footer a {
  color: inherit;
  text-decoration: underline;
}
.font-125 {
  font-size: 125%;
}
.tag-btn {
  margin-bottom: 0.3em;
}
pre {
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
  padding: 16px;
}
pre code {
  padding: 0;
  font-size: inherit;
  color: inherit;
  background-color: transparent;
  border-radius: 0;
}
code {
  padding: 2px 4px;
  font-size: 90%;
  color: #c7254e;
  background-color: #f9f2f4;
  border-radius: 4px;
}
img,
iframe,
embed,
video,
audio {
  max-width: 100%;
}
.card-img,
.card-img-top,
.card-img-bottom {
  width: initial;
}
</style>

</head>

  <body class="d-flex flex-column h-100">
    <div id="nav-border" class="container">
  <nav class="navbar navbar-expand-lg navbar-light justify-content-center">
    
    
    <ul class="navbar-nav">
      
        
        
        
        
        
        

        
          <li class="nav-item ">
            <a class="nav-link" href="/">
              
                
                <i data-feather="home"></i> 
              
              Home
            </a>
          </li>
        
      
        
        
        
        
        
        

        
          <li class="nav-item active">
            <a class="nav-link" href="/post/">
              
                
                <i data-feather="file-text"></i> 
              
              Blog
            </a>
          </li>
        
      
        
        
        
        
        
        

        
          <li class="nav-item ">
            <a class="nav-link" href="/learn/">
              
                
                <i data-feather="book-open"></i> 
              
              Learn
            </a>
          </li>
        
      
        
        
        
        
        
        

        
          <li class="nav-item ">
            <a class="nav-link" href="/cuda/">
              
              NVIDIA CUDA
            </a>
          </li>
        
      
        
        
        
        
        
        

        
          <li class="nav-item ">
            <a class="nav-link" href="/rocm/">
              
              AMD ROCm
            </a>
          </li>
        
      
        
        
        
        
        
        

        
          <li class="nav-item ">
            <a class="nav-link" href="/other/">
              
              Other
            </a>
          </li>
        
      
    </ul>
  </nav>
</div>

    <div class="container">
      <main id="main">
        

<h1>Paper: Flexible Performant GEMM Kernels on GPUs</h1>



<i data-feather="calendar"></i> <time datetime="2020-09-28">Sep 28, 2020</time>




  <br>
  <i data-feather="edit-2"></i> Thomas Faingnaert, Tim Besard, Bjorn De Sutter


<br><br>

    <p>General Matrix Multiplication or GEMM kernels take center place in high performance
computing and machine learning. Recent NVIDIA GPUs include GEMM accelerators, such as
NVIDIA&rsquo;s Tensor Cores. In this paper we show how it is possible to program these
accelerators from Julia, and present abstractions and interfaces that allow to do so
efficiently without sacrificing performance.</p>
<p>A pre-print of the paper has been published on arXiv:
<a href="https://arxiv.org/abs/2009.12263">arXiv:2009.12263</a>. <br> The source code can be found on
GitHub:
<a href="https://github.com/thomasfaingnaert/GemmKernels.jl">thomasfaingnaert/GemmKernels.jl</a>.</p>
<p>With the APIs from GemmKernels.jl, it is possible to instantiate GEMM kernels that perform
in the same ball park as, and sometimes even outperform state-of-the-art libraries like
CUBLAS and CUTLASS. For example, performing a mixed-precision multiplication of two 16-bit
matrixes into a 32-bit accumulator (on different combinations of layouts):</p>


<figure>
	<img src="https://juliagpu.org/2020-09-28-gemmkernels/mixed_precision.png" alt="Performance of mixed-precision GEMM" />
</figure>

<p>The APIs are also highly flexible and allow customization of each step, e.g., to apply the
activation function <code>max(x, 0)</code> for implementing a rectified linear unit (ReLU):</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-julia" data-lang="julia">a <span style="color:#f92672">=</span> CuArray(rand(<span style="color:#66d9ef">Float16</span>, (M, K)))
b <span style="color:#f92672">=</span> CuArray(rand(<span style="color:#66d9ef">Float16</span>, (K, N)))
c <span style="color:#f92672">=</span> CuArray(rand(<span style="color:#66d9ef">Float32</span>, (M, N)))
d <span style="color:#f92672">=</span> similar(c)

conf <span style="color:#f92672">=</span> GemmKernels<span style="color:#f92672">.</span>get_config(
    gemm_shape <span style="color:#f92672">=</span> (M <span style="color:#f92672">=</span> M, N <span style="color:#f92672">=</span> N, K <span style="color:#f92672">=</span> K),
    operator <span style="color:#f92672">=</span> Operator<span style="color:#f92672">.</span>WMMAOp{<span style="color:#ae81ff">16</span>, <span style="color:#ae81ff">16</span>, <span style="color:#ae81ff">16</span>},
    global_a_layout <span style="color:#f92672">=</span> Layout<span style="color:#f92672">.</span>AlignedColMajor{<span style="color:#66d9ef">Float16</span>},
    global_c_layout <span style="color:#f92672">=</span> Layout<span style="color:#f92672">.</span>AlignedColMajor{<span style="color:#66d9ef">Float32</span>})

GemmKernels<span style="color:#f92672">.</span>matmul(
    a, b, c, d, conf;
    transform_regs_to_shared_d <span style="color:#f92672">=</span> Transform<span style="color:#f92672">.</span>Elementwise(x <span style="color:#f92672">-&gt;</span> max(x, <span style="color:#ae81ff">0</span>)))
</code></pre></div><p>The GemmKernels.jl framework is written entirely in Julia, demonstrating the
high-performance GPU programming capabilities of this language, but at the same time keeping
the research accessible and easy to modify or repurpose by other Julia developers.</p>



      </main>
    </div>
    
<footer id="footer" class="mt-auto text-center text-muted">
  <div class="container">
    Made with <a href="https://gohugo.io/">Hugo</a> &amp; <a href="https://github.com/zwbetz-gh/vanilla-bootstrap-hugo-theme">Vanilla</a>
  </div>
</footer>

    <script src="https://juliagpu.org/js/feather.min.js"></script>
<script>
  feather.replace()
</script>


    
  
  <script>
  window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
  ga('create', 'UA-154489943-1', 'auto');
  ga('send', 'pageview');
  </script>
  <script async src='https://www.google-analytics.com/analytics.js'></script>
  

  </body>
</html>
