<!doctype html>

<html lang="en" class="h-100">
  <head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta content="index, follow" name="robots">
  <meta name="generator" content="Hugo 0.77.0" />
  <link rel="stylesheet" href="https://juliagpu.org/css/bootstrap.min.css">

  
  
  
  <title>Introducing: oneAPI.jl · JuliaGPU</title>
  <style>
.container {
  max-width: 700px;
}
#nav-border {
  border-bottom: 1px solid #212529;
}
#main {
  margin-top: 1em;
  margin-bottom: 4em;
}
#home-jumbotron {
  background-color: inherit;
}
#footer .container {
  padding: 1em 0;
}
#footer a {
  color: inherit;
  text-decoration: underline;
}
.font-125 {
  font-size: 125%;
}
.tag-btn {
  margin-bottom: 0.3em;
}
pre {
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 4px;
  padding: 16px;
}
pre code {
  padding: 0;
  font-size: inherit;
  color: inherit;
  background-color: transparent;
  border-radius: 0;
}
code {
  padding: 2px 4px;
  font-size: 90%;
  color: #c7254e;
  background-color: #f9f2f4;
  border-radius: 4px;
}
img,
iframe,
embed,
video,
audio {
  max-width: 100%;
}
.card-img,
.card-img-top,
.card-img-bottom {
  width: initial;
}
</style>

</head>

  <body class="d-flex flex-column h-100">
    <div id="nav-border" class="container">
  <nav class="navbar navbar-expand-lg navbar-light justify-content-center">
    
    
    <ul class="navbar-nav">
      
        
        
        
        
        
        

        
          <li class="nav-item ">
            <a class="nav-link" href="/">
              
                
                <i data-feather="home"></i> 
              
              Home
            </a>
          </li>
        
      
        
        
        
        
        
        

        
          <li class="nav-item active">
            <a class="nav-link" href="/post/">
              
                
                <i data-feather="file-text"></i> 
              
              Blog
            </a>
          </li>
        
      
        
        
        
        
        
        

        
          <li class="nav-item ">
            <a class="nav-link" href="/learn/">
              
                
                <i data-feather="book-open"></i> 
              
              Learn
            </a>
          </li>
        
      
        
        
        
        
        
        

        
          <li class="nav-item ">
            <a class="nav-link" href="/cuda/">
              
              NVIDIA CUDA
            </a>
          </li>
        
      
        
        
        
        
        
        

        
          <li class="nav-item ">
            <a class="nav-link" href="/rocm/">
              
              AMD ROCm
            </a>
          </li>
        
      
        
        
        
        
        
        

        
          <li class="nav-item ">
            <a class="nav-link" href="/other/">
              
              Other
            </a>
          </li>
        
      
    </ul>
  </nav>
</div>

    <div class="container">
      <main id="main">
        

<h1>Introducing: oneAPI.jl</h1>



<i data-feather="calendar"></i> <time datetime="2020-11-05">Nov 5, 2020</time>




  <br>
  <i data-feather="edit-2"></i> Tim Besard


<br><br>

    <p>We&rsquo;re proud to announce the first version of oneAPI.jl, a Julia package for programming
accelerators with the <a href="https://www.oneapi.com/">oneAPI programming model</a>. It is currently
available for select Intel GPUs, including common integrated ones, and offers a similar
experience to CUDA.jl.</p>
<p>The initial version of this package, v0.1, consists of three key components:</p>
<ul>
<li>wrappers for the oneAPI Level Zero interfaces;</li>
<li>a compiler for Julia source code to SPIR-V IR;</li>
<li>and an array interface for convenient data-parallel programming.</li>
</ul>
<p>In this post, I&rsquo;ll briefly describe each of these. But first, some essentials.</p>
<h2 id="installation">Installation</h2>
<p>oneAPI.jl is currently only supported on 64-bit Linux, using a sufficiently recent kernel,
and requires Julia 1.5. Furthermore, it currently only supports a limited set of Intel GPUs:
Gen9 (Skylake, Kaby Lake, Coffee Lake), Gen11 (Ice Lake), and Gen12 (Tiger Lake).</p>
<p>If your Intel CPU has an integrated GPU supported by oneAPI, you can just go ahead and
install the oneAPI.jl package:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-julia" data-lang="julia">pkg<span style="color:#f92672">&gt;</span> add oneAPI
</code></pre></div><p>That&rsquo;s right, no additional drivers required! oneAPI.jl ships its own copy of the <a href="https://github.com/intel/compute-runtime">Intel
Compute Runtime</a>, which works out of the box on
any (sufficiently recent) Linux kernel. The initial download, powered by Julia&rsquo;s artifact
subsystem, might take a while to complete. After that, you can import the package and start
using its functionality:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-julia" data-lang="julia">julia<span style="color:#f92672">&gt;</span> <span style="color:#66d9ef">using</span> oneAPI

julia<span style="color:#f92672">&gt;</span> oneAPI<span style="color:#f92672">.</span>versioninfo()
Binary dependencies<span style="color:#f92672">:</span>
<span style="color:#f92672">-</span> NEO_jll<span style="color:#f92672">:</span> <span style="color:#ae81ff">20.42</span><span style="color:#f92672">.</span><span style="color:#ae81ff">18209</span><span style="color:#f92672">+</span><span style="color:#ae81ff">0</span>
<span style="color:#f92672">-</span> libigc_jll<span style="color:#f92672">:</span> <span style="color:#ae81ff">1.0</span><span style="color:#f92672">.</span><span style="color:#ae81ff">5186</span><span style="color:#f92672">+</span><span style="color:#ae81ff">0</span>
<span style="color:#f92672">-</span> gmmlib_jll<span style="color:#f92672">:</span> <span style="color:#ae81ff">20.3</span><span style="color:#f92672">.</span><span style="color:#ae81ff">2</span><span style="color:#f92672">+</span><span style="color:#ae81ff">0</span>
<span style="color:#f92672">-</span> SPIRV_LLVM_Translator_jll<span style="color:#f92672">:</span> <span style="color:#ae81ff">9.0</span><span style="color:#f92672">.</span><span style="color:#ae81ff">0</span><span style="color:#f92672">+</span><span style="color:#ae81ff">1</span>
<span style="color:#f92672">-</span> SPIRV_Tools_jll<span style="color:#f92672">:</span> <span style="color:#ae81ff">2020.2</span><span style="color:#f92672">.</span><span style="color:#ae81ff">0</span><span style="color:#f92672">+</span><span style="color:#ae81ff">1</span>

Toolchain<span style="color:#f92672">:</span>
<span style="color:#f92672">-</span> Julia<span style="color:#f92672">:</span> <span style="color:#ae81ff">1.5</span><span style="color:#f92672">.</span><span style="color:#ae81ff">2</span>
<span style="color:#f92672">-</span> LLVM<span style="color:#f92672">:</span> <span style="color:#ae81ff">9.0</span><span style="color:#f92672">.</span><span style="color:#ae81ff">1</span>

<span style="color:#ae81ff">1</span> driver<span style="color:#f92672">:</span>
<span style="color:#f92672">-</span> <span style="color:#ae81ff">00007</span>fee<span style="color:#f92672">-</span><span style="color:#ae81ff">06</span>cb<span style="color:#f92672">-</span><span style="color:#ae81ff">0</span>a10<span style="color:#f92672">-</span><span style="color:#ae81ff">1642</span><span style="color:#f92672">-</span>ca9f01000000 (v1<span style="color:#f92672">.</span><span style="color:#ae81ff">0.0</span>, API v1<span style="color:#f92672">.</span><span style="color:#ae81ff">0.0</span>)

<span style="color:#ae81ff">1</span> device<span style="color:#f92672">:</span>
<span style="color:#f92672">-</span> Intel(R) Graphics Gen9
</code></pre></div><h2 id="the-onearray-type">The <code>oneArray</code> type</h2>
<p>Similar to CUDA.jl&rsquo;s <code>CuArray</code> type, oneAPI.jl provides an array abstraction that you can
use to easily perform data parallel operations on your GPU:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-julia" data-lang="julia">julia<span style="color:#f92672">&gt;</span> a <span style="color:#f92672">=</span> oneArray(zeros(<span style="color:#ae81ff">2</span>,<span style="color:#ae81ff">3</span>))
<span style="color:#ae81ff">2</span>×3 oneArray{<span style="color:#66d9ef">Float64</span>,<span style="color:#ae81ff">2</span>}<span style="color:#f92672">:</span>
 <span style="color:#ae81ff">0.0</span>  <span style="color:#ae81ff">0.0</span>  <span style="color:#ae81ff">0.0</span>
 <span style="color:#ae81ff">0.0</span>  <span style="color:#ae81ff">0.0</span>  <span style="color:#ae81ff">0.0</span>

julia<span style="color:#f92672">&gt;</span> a <span style="color:#f92672">.+</span> <span style="color:#ae81ff">1</span>
<span style="color:#ae81ff">2</span>×3 oneArray{<span style="color:#66d9ef">Float64</span>,<span style="color:#ae81ff">2</span>}<span style="color:#f92672">:</span>
 <span style="color:#ae81ff">1.0</span>  <span style="color:#ae81ff">1.0</span>  <span style="color:#ae81ff">1.0</span>
 <span style="color:#ae81ff">1.0</span>  <span style="color:#ae81ff">1.0</span>  <span style="color:#ae81ff">1.0</span>

julia<span style="color:#f92672">&gt;</span> sum(ans; dims<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>)
<span style="color:#ae81ff">2</span>×1 oneArray{<span style="color:#66d9ef">Float64</span>,<span style="color:#ae81ff">2</span>}<span style="color:#f92672">:</span>
 <span style="color:#ae81ff">3.0</span>
 <span style="color:#ae81ff">3.0</span>
</code></pre></div><p>This functionality builds on the <a href="https://github.com/JuliaGPU/GPUArrays.jl/">GPUArrays.jl</a>
package, which means that a lot of operations are supported out of the box. Some are still
missing, of course, and we haven&rsquo;t carefully optimized for performance either.</p>
<h2 id="kernel-programming">Kernel programming</h2>
<p>The above array operations are made possible by a compiler that transforms Julia source code
into SPIR-V IR for use with oneAPI. Most of this work is part of
<a href="https://github.com/JuliaGPU/GPUCompiler.jl">GPUCompiler.jl</a>. In oneAPI.jl, we use this
compiler to provide a kernel programming model:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-julia" data-lang="julia">julia<span style="color:#f92672">&gt;</span> <span style="color:#66d9ef">function</span> vadd(a, b, c)
           i <span style="color:#f92672">=</span> get_global_id()
           <span style="color:#a6e22e">@inbounds</span> c[i] <span style="color:#f92672">=</span> a[i] <span style="color:#f92672">+</span> b[i]
           <span style="color:#66d9ef">return</span>
       <span style="color:#66d9ef">end</span>

julia<span style="color:#f92672">&gt;</span> a <span style="color:#f92672">=</span> oneArray(rand(<span style="color:#ae81ff">10</span>));

julia<span style="color:#f92672">&gt;</span> b <span style="color:#f92672">=</span> oneArray(rand(<span style="color:#ae81ff">10</span>));

julia<span style="color:#f92672">&gt;</span> c <span style="color:#f92672">=</span> similar(a);

julia<span style="color:#f92672">&gt;</span> <span style="color:#a6e22e">@oneapi</span> items<span style="color:#f92672">=</span><span style="color:#ae81ff">10</span> vadd(a, b, c)

julia<span style="color:#f92672">&gt;</span> <span style="color:#a6e22e">@test</span> <span style="color:#66d9ef">Array</span>(a) <span style="color:#f92672">.+</span> <span style="color:#66d9ef">Array</span>(b) <span style="color:#f92672">==</span> <span style="color:#66d9ef">Array</span>(c)
Test Passed
</code></pre></div><p>Again, the <code>@oneapi</code> macro resembles <code>@cuda</code> from CUDA.jl. One of the differences with the
CUDA stack is that we use OpenCL-style built-ins, like <code>get_global_id</code> instead of
<code>threadIdx</code> and <code>barrier</code> instead of <code>sync_threads</code>. Other familiar functionality, e.g. to
reflect on the compiler, is available as well:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-julia" data-lang="julia">julia<span style="color:#f92672">&gt;</span> <span style="color:#a6e22e">@device_code_spirv</span> <span style="color:#a6e22e">@oneapi</span> vadd(a, b, c)
; CompilerJob of kernel vadd(oneDeviceArray{<span style="color:#66d9ef">Float64</span>,<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>}, oneDeviceArray{<span style="color:#66d9ef">Float64</span>,<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>},
; oneDeviceArray{<span style="color:#66d9ef">Float64</span>,<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>}) <span style="color:#66d9ef">for</span> GPUCompiler<span style="color:#f92672">.</span>SPIRVCompilerTarget

; SPIR<span style="color:#f92672">-</span>V
; Version<span style="color:#f92672">:</span> <span style="color:#ae81ff">1.0</span>
; Generator<span style="color:#f92672">:</span> Khronos LLVM<span style="color:#f92672">/</span>SPIR<span style="color:#f92672">-</span>V Translator; <span style="color:#ae81ff">14</span>
; Bound<span style="color:#f92672">:</span> <span style="color:#ae81ff">46</span>
; Schema<span style="color:#f92672">:</span> <span style="color:#ae81ff">0</span>
               OpCapability Addresses
               OpCapability Linkage
               OpCapability Kernel
               OpCapability <span style="color:#66d9ef">Float64</span>
               OpCapability <span style="color:#66d9ef">Int64</span>
               OpCapability <span style="color:#66d9ef">Int8</span>
          <span style="color:#f92672">%</span><span style="color:#ae81ff">1</span> <span style="color:#f92672">=</span> OpExtInstImport <span style="color:#e6db74">&#34;OpenCL.std&#34;</span>
               OpMemoryModel Physical64 OpenCL
               OpEntryPoint Kernel
               <span style="color:#f92672">...</span>
               OpReturn
               OpFunctionEnd
</code></pre></div><h2 id="level-zero-wrappers">Level Zero wrappers</h2>
<p>To interface with the oneAPI driver, we use the <a href="https://github.com/oneapi-src/level-zero">Level Zero
API</a>. Wrappers for this API is available under the
<code>oneL0</code> submodule of oneAPI.jl:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-julia" data-lang="julia">julia<span style="color:#f92672">&gt;</span> <span style="color:#66d9ef">using</span> oneAPI<span style="color:#f92672">.</span>oneL0

julia<span style="color:#f92672">&gt;</span> drv <span style="color:#f92672">=</span> first(drivers())
ZeDriver(<span style="color:#ae81ff">00000000</span><span style="color:#f92672">-</span><span style="color:#ae81ff">0000</span><span style="color:#f92672">-</span><span style="color:#ae81ff">0000</span><span style="color:#f92672">-</span><span style="color:#ae81ff">1642</span><span style="color:#f92672">-</span>ca9f01000000, version <span style="color:#ae81ff">1.0</span><span style="color:#f92672">.</span><span style="color:#ae81ff">0</span>)

julia<span style="color:#f92672">&gt;</span> dev <span style="color:#f92672">=</span> first(devices(drv))
ZeDevice(GPU, vendor <span style="color:#ae81ff">0x8086</span>, device <span style="color:#ae81ff">0x1912</span>)<span style="color:#f92672">:</span> Intel(R) Graphics Gen9
</code></pre></div><p>This is a low-level interface, and importing this submodule should not be required for the
vast majority of users. It is only useful when you want to perform very specific operations,
like submitting an certain operations to the command queue, working with events, etc. In
that case, you should refer to the <a href="https://spec.oneapi.com/level-zero/latest/index.html">upstream
specification</a>; The wrappers in the
<code>oneL0</code> module closely mimic the C APIs.</p>
<h2 id="status">Status</h2>
<p>Version 0.1 of oneAPI.jl forms a solid base for future oneAPI developments in Julia. Thanks
to the continued effort of generalizing the Julia GPU support in packages like GPUArrays.jl
and GPUCompiler.jl, this initial version is already much more usable than early versions of
CUDA.jl or AMDGPU.jl ever were.</p>
<p>That said, there are crucial parts missing. For one, oneAPI.jl does not integrate with any
of the vendor libraries like oneMKL or oneDNN. That means several important operations, e.g.
matrix-matrix multiplication, will be slow. Hardware support is also limited, and the
package currently only works on Linux.</p>
<p>If you want to contribute to oneAPI.jl, or run into problems, check out the GitHub
repository at <a href="https://github.com/JuliaGPU/oneAPI.jl">JuliaGPU/oneAPI.jl</a>. For questions,
please use the <a href="https://discourse.julialang.org/c/domain/gpu">Julia Discourse forum</a> under
the GPU domain and/or in the #gpu channel of the <a href="https://julialang.org/community/">Julia
Slack</a>.</p>



      </main>
    </div>
    
<footer id="footer" class="mt-auto text-center text-muted">
  <div class="container">
    Made with <a href="https://gohugo.io/">Hugo</a> &amp; <a href="https://github.com/zwbetz-gh/vanilla-bootstrap-hugo-theme">Vanilla</a>
  </div>
</footer>

    <script src="https://juliagpu.org/js/feather.min.js"></script>
<script>
  feather.replace()
</script>


    
  
  <script>
  window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
  ga('create', 'UA-154489943-1', 'auto');
  ga('send', 'pageview');
  </script>
  <script async src='https://www.google-analytics.com/analytics.js'></script>
  

  </body>
</html>
